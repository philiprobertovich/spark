{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Currency']\n",
      "StructType([StructField('Date', StringType(), True), StructField('Open', StringType(), True), StructField('High', StringType(), True), StructField('Low', StringType(), True), StructField('Close', StringType(), True), StructField('Volume', StringType(), True), StructField('Currency', StringType(), True)])\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# Create path and file variables\n",
    "data_dir = './data'\n",
    "coffee_data = 'coffee.csv'\n",
    "\n",
    "# Create coffee PySpark DataFrame\n",
    "coffee_df = sparkql.read.csv(os.path.join(data_dir, coffee_data), header=True)\n",
    "\n",
    "# Show columns, schema, and df\n",
    "print(coffee_df.columns)\n",
    "print(coffee_df.schema)\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2000-01-03|\n",
      "|2000-01-04|\n",
      "|2000-01-05|\n",
      "|2000-01-06|\n",
      "+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "StructType([StructField('Date', DateType(), True), StructField('Open', FloatType(), True), StructField('High', FloatType(), True), StructField('Low', FloatType(), True), StructField('Close', FloatType(), True), StructField('Volume', IntegerType(), True), StructField('Currency', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Change the data types of columns\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Cast Date column as DateType with to_date method\n",
    "coffee_df = coffee_df.withColumn('Date', to_date(coffee_df['Date'], format='yyyy-MM-dd'))\n",
    "# Make sure date is properly formatted\n",
    "coffee_df.select('Date').show(4)\n",
    "\n",
    "# Cast other columns as FloatType\n",
    "coffee_df = coffee_df.withColumn('Open', coffee_df['Open'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('High', coffee_df['High'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Low', coffee_df['Low'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Close', coffee_df['Close'].cast(FloatType()))\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Volume', coffee_df['Volume'].cast(IntegerType()))\n",
    "\n",
    "print(coffee_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create High-Low and Open-Close difference Columns\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Diff', round(coffee_df.Open - coffee_df.Close, 2))\n",
    "coffee_df = coffee_df.withColumn('High_Low_Diff', round(coffee_df.High - coffee_df.Low, 2))\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|             true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|             true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|             true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|             true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|  6855|     USD|            3.1|         3.95|             true|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-03-08|108.75|108.75| 107.0| 107.4|    67|     USD|           1.35|         1.75|            false|\n",
      "|2000-03-09|108.25| 109.0| 108.0| 107.2|    93|     USD|           1.05|          1.0|            false|\n",
      "|2000-03-13| 106.5| 106.5|105.75| 106.0|    91|     USD|            0.5|         0.75|            false|\n",
      "|2000-03-21| 103.5| 104.0| 103.5|103.75|     0|     USD|          -0.25|          0.5|            false|\n",
      "|2000-05-11|  96.0|  97.0|  95.1|  95.9|    37|     USD|            0.1|          1.9|            false|\n",
      "|2000-05-15| 92.25| 92.75| 91.25| 91.25|    67|     USD|            1.0|          1.5|            false|\n",
      "|2000-07-10|  83.5|  85.5|  83.5|  84.0|    46|     USD|           -0.5|          2.0|            false|\n",
      "|2000-07-11|  83.0| 91.15|  84.5|  90.1|    41|     USD|           -7.1|         6.65|            false|\n",
      "|2000-07-12|  90.5|  90.5|  88.8|  89.7|    12|     USD|            0.8|          1.7|            false|\n",
      "|2000-07-13|  93.0|  99.5|  93.0|  99.2|    20|     USD|           -6.2|          6.5|            false|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "# Create Boolean column based on Volume values\n",
    "coffee_df = coffee_df.withColumn('volume_filter_100', when(coffee_df.Volume >= 100, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# Making sure column was properly created\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == True).show(5)\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b317e639739deb127efa3a357daeb3681244e8477c1f3da0e4c6730da0730415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
