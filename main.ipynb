{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Currency']\n",
      "StructType([StructField('Date', StringType(), True), StructField('Open', StringType(), True), StructField('High', StringType(), True), StructField('Low', StringType(), True), StructField('Close', StringType(), True), StructField('Volume', StringType(), True), StructField('Currency', StringType(), True)])\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# Create path and file variables\n",
    "data_dir = './data'\n",
    "coffee_data = 'coffee.csv'\n",
    "\n",
    "# Create coffee PySpark DataFrame\n",
    "coffee_df = sparkql.read.csv(os.path.join(data_dir, coffee_data), header=True)\n",
    "\n",
    "# Show columns, schema, and df\n",
    "print(coffee_df.columns)\n",
    "print(coffee_df.schema)\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2000-01-03|\n",
      "|2000-01-04|\n",
      "|2000-01-05|\n",
      "|2000-01-06|\n",
      "+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "StructType([StructField('Date', DateType(), True), StructField('Open', FloatType(), True), StructField('High', FloatType(), True), StructField('Low', FloatType(), True), StructField('Close', FloatType(), True), StructField('Volume', IntegerType(), True), StructField('Currency', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Change the data types of columns\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Cast Date column as DateType with to_date method\n",
    "coffee_df = coffee_df.withColumn('Date', to_date(coffee_df['Date'], format='yyyy-MM-dd'))\n",
    "# Make sure date is properly formatted\n",
    "coffee_df.select('Date').show(4)\n",
    "\n",
    "# Cast other columns as FloatType\n",
    "coffee_df = coffee_df.withColumn('Open', coffee_df['Open'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('High', coffee_df['High'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Low', coffee_df['Low'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Close', coffee_df['Close'].cast(FloatType()))\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Volume', coffee_df['Volume'].cast(IntegerType()))\n",
    "\n",
    "print(coffee_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create Aggregate Columns\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Diff', round(coffee_df.Open - coffee_df.Close, 2))\n",
    "coffee_df = coffee_df.withColumn('High_Low_Diff', round(coffee_df.High - coffee_df.Low, 2))\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b317e639739deb127efa3a357daeb3681244e8477c1f3da0e4c6730da0730415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
