{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Currency']\n",
      "StructType([StructField('Date', StringType(), True), StructField('Open', StringType(), True), StructField('High', StringType(), True), StructField('Low', StringType(), True), StructField('Close', StringType(), True), StructField('Volume', StringType(), True), StructField('Currency', StringType(), True)])\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# Create path and file variables\n",
    "data_dir = './data'\n",
    "coffee_data = 'coffee.csv'\n",
    "\n",
    "# Create coffee PySpark DataFrame\n",
    "coffee_df = sparkql.read.csv(os.path.join(data_dir, coffee_data), header=True)\n",
    "\n",
    "# Show columns, schema, and df\n",
    "print(coffee_df.columns)\n",
    "print(coffee_df.schema)\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2000-01-03|\n",
      "|2000-01-04|\n",
      "|2000-01-05|\n",
      "|2000-01-06|\n",
      "+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "StructType([StructField('Date', DateType(), True), StructField('Open', FloatType(), True), StructField('High', FloatType(), True), StructField('Low', FloatType(), True), StructField('Close', FloatType(), True), StructField('Volume', IntegerType(), True), StructField('Currency', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Change the data types of columns\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Cast Date column as DateType with to_date method\n",
    "coffee_df = coffee_df.withColumn('Date', to_date(coffee_df['Date'], format='yyyy-MM-dd'))\n",
    "# Make sure date is properly formatted\n",
    "coffee_df.select('Date').show(4)\n",
    "\n",
    "# Cast other columns as FloatType\n",
    "coffee_df = coffee_df.withColumn('Open', coffee_df['Open'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('High', coffee_df['High'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Low', coffee_df['Low'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Close', coffee_df['Close'].cast(FloatType()))\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Volume', coffee_df['Volume'].cast(IntegerType()))\n",
    "\n",
    "print(coffee_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create High-Low and Open-Close difference Columns\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Diff', round(coffee_df.Open - coffee_df.Close, 2))\n",
    "coffee_df = coffee_df.withColumn('High_Low_Diff', round(coffee_df.High - coffee_df.Low, 2))\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|             true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|             true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|             true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|             true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|  6855|     USD|            3.1|         3.95|             true|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-03-08|108.75|108.75| 107.0| 107.4|    67|     USD|           1.35|         1.75|            false|\n",
      "|2000-03-09|108.25| 109.0| 108.0| 107.2|    93|     USD|           1.05|          1.0|            false|\n",
      "|2000-03-13| 106.5| 106.5|105.75| 106.0|    91|     USD|            0.5|         0.75|            false|\n",
      "|2000-03-21| 103.5| 104.0| 103.5|103.75|     0|     USD|          -0.25|          0.5|            false|\n",
      "|2000-05-11|  96.0|  97.0|  95.1|  95.9|    37|     USD|            0.1|          1.9|            false|\n",
      "|2000-05-15| 92.25| 92.75| 91.25| 91.25|    67|     USD|            1.0|          1.5|            false|\n",
      "|2000-07-10|  83.5|  85.5|  83.5|  84.0|    46|     USD|           -0.5|          2.0|            false|\n",
      "|2000-07-11|  83.0| 91.15|  84.5|  90.1|    41|     USD|           -7.1|         6.65|            false|\n",
      "|2000-07-12|  90.5|  90.5|  88.8|  89.7|    12|     USD|            0.8|          1.7|            false|\n",
      "|2000-07-13|  93.0|  99.5|  93.0|  99.2|    20|     USD|           -6.2|          6.5|            false|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "# Create Boolean column based on Volume values\n",
    "coffee_df = coffee_df.withColumn('volume_filter_100', when(coffee_df.Volume >= 100, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# Making sure column was properly created\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == True).show(5)\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Open_Close_Abs|\n",
      "+--------------+\n",
      "|          5.75|\n",
      "|           0.0|\n",
      "|           3.6|\n",
      "|          2.15|\n",
      "|           3.1|\n",
      "|          5.95|\n",
      "|           2.3|\n",
      "|          1.15|\n",
      "|           0.7|\n",
      "|           5.2|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Absolute Value column\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Abs', abs(coffee_df.Open_Close_Diff))\n",
    "\n",
    "# Show Absolute Value column\n",
    "coffee_df.select('Open_Close_Abs').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        net_sales|\n",
      "+-----------------+\n",
      "|         794808.0|\n",
      "|         643662.4|\n",
      "|         723771.0|\n",
      "|         603129.6|\n",
      "|         793123.5|\n",
      "|906629.1000000001|\n",
      "|         464396.8|\n",
      "|         614304.0|\n",
      "|         441579.6|\n",
      "|        1170305.5|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# # Create udf\n",
    "# @pandas_udf(float)\n",
    "# def compute_net(c1: pd.Series,\n",
    "#                 c2: pd.Series,\n",
    "#                 c3: pd.Series, \n",
    "#                 c4: pd.Series, \n",
    "#                 c5: pd.Series) -> float:\n",
    "#   return ((c1 + c2 + c3 + c4) / 4) * c5\n",
    "\n",
    "# # Create Winow\n",
    "# w = Window \\\n",
    "#     .partitionBy('Open', 'High', 'Close', 'Low') \\\n",
    "#     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Create net_values column\n",
    "columns_list = [col('Open'), col('High'), col('Close'), col('Low')]\n",
    "avg_func = round(sum(x for x in columns_list)/ len(columns_list), 1)\n",
    "\n",
    "coffee_df = coffee_df.withColumn('net_sales', avg_func * coffee_df.Volume)\n",
    "\n",
    "# Make sure net_values column is there \n",
    "coffee_df.select('net_sales').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|round(avg(Open_Close_Abs), 2)|\n",
      "+-----------------------------+\n",
      "|                         1.76|\n",
      "+-----------------------------+\n",
      "\n",
      "1638\n",
      "+-------------------+\n",
      "|round(avg(Open), 2)|\n",
      "+-------------------+\n",
      "|             126.05|\n",
      "+-------------------+\n",
      "\n",
      "+---------+\n",
      "|max(High)|\n",
      "+---------+\n",
      "|   306.25|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Find stats\n",
    "coffee_df.select(round(avg('Open_Close_Abs'), 2)).show()\n",
    "print(coffee_df.select(coffee_df.Volume).where(coffee_df.Volume < 100).count())\n",
    "coffee_df.select(round(avg('Open'), 2)).show()\n",
    "coffee_df.select(max(coffee_df.High)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write parquet file\n",
    "coffee_df.write.parquet('./data/coffee_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+--------------+------------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|Open_Close_Abs|         net_sales|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+--------------+------------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|             true|          5.75|          794808.0|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|             true|           0.0|          643662.4|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|             true|           3.6|          723771.0|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|             true|          2.15|          603129.6|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|  6855|     USD|            3.1|         3.95|             true|           3.1|          793123.5|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55|  7499|     USD|           5.95|          9.3|             true|          5.95| 906629.1000000001|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8|  3976|     USD|           -2.3|         2.75|             true|           2.3|          464396.8|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95|  5184|     USD|          -1.15|          3.6|             true|          1.15|          614304.0|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55|  3717|     USD|            0.7|          2.5|             true|           0.7|          441579.6|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55| 10115|     USD|            5.2|          8.0|             true|           5.2|         1170305.5|\n",
      "|2000-01-18|111.75|118.25| 110.6|115.75|  7364|     USD|           -4.0|         7.65|             true|           4.0| 840232.3999999999|\n",
      "|2000-01-19| 116.5|118.25|114.75| 116.7|  6626|     USD|           -0.2|          3.5|             true|           0.2|          772591.6|\n",
      "|2000-01-20|118.25| 118.8| 111.7| 112.0|  8834|     USD|           6.25|          7.1|             true|          6.25|         1017676.8|\n",
      "|2000-01-21| 112.0| 113.5| 110.8| 111.2|  5625|     USD|            0.8|          2.7|             true|           0.8|          629437.5|\n",
      "|2000-01-24|110.95| 114.4|110.95| 111.9|  5821|     USD|          -0.95|         3.45|             true|          0.95|          652534.1|\n",
      "|2000-01-25| 111.6| 113.7| 111.6|112.85|  4014|     USD|          -1.25|          2.1|             true|          1.25|451173.60000000003|\n",
      "|2000-01-26| 112.5| 115.3| 111.9|115.15|  5796|     USD|          -2.65|          3.4|             true|          2.65| 659005.2000000001|\n",
      "|2000-01-27|114.75| 116.4| 112.8| 114.6|  5477|     USD|           0.15|          3.6|             true|          0.15|          627664.2|\n",
      "|2000-01-28| 115.1| 115.4| 113.7| 114.7|  3334|     USD|            0.4|          1.7|             true|           0.4|          382409.8|\n",
      "|2000-01-31|113.75| 114.0| 110.5| 111.1|  6465|     USD|           2.65|          3.5|             true|          2.65|          726019.5|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26c689f96b8e72689cf4d564706b2043266f0c8b4fc99948bbcdd069e08b7669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
