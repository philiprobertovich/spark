{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Currency']\n",
      "StructType([StructField('Date', StringType(), True), StructField('Open', StringType(), True), StructField('High', StringType(), True), StructField('Low', StringType(), True), StructField('Close', StringType(), True), StructField('Volume', StringType(), True), StructField('Currency', StringType(), True)])\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|\n",
      "+----------+------+-----+------+------+------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# Create path and file variables\n",
    "data_dir = './data'\n",
    "coffee_data = 'coffee.csv'\n",
    "\n",
    "# Create coffee PySpark DataFrame\n",
    "coffee_df = sparkql.read.csv(os.path.join(data_dir, coffee_data), header=True)\n",
    "\n",
    "# Show columns, schema, and df\n",
    "print(coffee_df.columns)\n",
    "print(coffee_df.schema)\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2000-01-03|\n",
      "|2000-01-04|\n",
      "|2000-01-05|\n",
      "|2000-01-06|\n",
      "+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "StructType([StructField('Date', DateType(), True), StructField('Open', FloatType(), True), StructField('High', FloatType(), True), StructField('Low', FloatType(), True), StructField('Close', FloatType(), True), StructField('Volume', IntegerType(), True), StructField('Currency', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Change the data types of columns\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Cast Date column as DateType with to_date method\n",
    "coffee_df = coffee_df.withColumn('Date', to_date(coffee_df['Date'], format='yyyy-MM-dd'))\n",
    "# Make sure date is properly formatted\n",
    "coffee_df.select('Date').show(4)\n",
    "\n",
    "# Cast other columns as FloatType\n",
    "coffee_df = coffee_df.withColumn('Open', coffee_df['Open'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('High', coffee_df['High'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Low', coffee_df['Low'].cast(FloatType()))\n",
    "coffee_df = coffee_df.withColumn('Close', coffee_df['Close'].cast(FloatType()))\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Volume', coffee_df['Volume'].cast(IntegerType()))\n",
    "\n",
    "print(coffee_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|      Date|  Open| High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "|2000-01-03|122.25|124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|\n",
      "|2000-01-04|116.25|120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|\n",
      "|2000-01-05| 115.0|121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|\n",
      "|2000-01-06| 119.0|121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|\n",
      "+----------+------+-----+------+------+------+--------+---------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Create High-Low and Open-Close difference Columns\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Diff', round(coffee_df.Open - coffee_df.Close, 2))\n",
    "coffee_df = coffee_df.withColumn('High_Low_Diff', round(coffee_df.High - coffee_df.Low, 2))\n",
    "coffee_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5|  6640|     USD|           5.75|          7.9|             true|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25|  5492|     USD|            0.0|         4.75|             true|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6|  6165|     USD|           -3.6|          6.0|             true|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85|  5094|     USD|           2.15|          4.9|             true|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15|  6855|     USD|            3.1|         3.95|             true|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|      Date|  Open|  High|   Low| Close|Volume|Currency|Open_Close_Diff|High_Low_Diff|volume_filter_100|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "|2000-03-08|108.75|108.75| 107.0| 107.4|    67|     USD|           1.35|         1.75|            false|\n",
      "|2000-03-09|108.25| 109.0| 108.0| 107.2|    93|     USD|           1.05|          1.0|            false|\n",
      "|2000-03-13| 106.5| 106.5|105.75| 106.0|    91|     USD|            0.5|         0.75|            false|\n",
      "|2000-03-21| 103.5| 104.0| 103.5|103.75|     0|     USD|          -0.25|          0.5|            false|\n",
      "|2000-05-11|  96.0|  97.0|  95.1|  95.9|    37|     USD|            0.1|          1.9|            false|\n",
      "|2000-05-15| 92.25| 92.75| 91.25| 91.25|    67|     USD|            1.0|          1.5|            false|\n",
      "|2000-07-10|  83.5|  85.5|  83.5|  84.0|    46|     USD|           -0.5|          2.0|            false|\n",
      "|2000-07-11|  83.0| 91.15|  84.5|  90.1|    41|     USD|           -7.1|         6.65|            false|\n",
      "|2000-07-12|  90.5|  90.5|  88.8|  89.7|    12|     USD|            0.8|          1.7|            false|\n",
      "|2000-07-13|  93.0|  99.5|  93.0|  99.2|    20|     USD|           -6.2|          6.5|            false|\n",
      "+----------+------+------+------+------+------+--------+---------------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "# Create Boolean column based on Volume values\n",
    "coffee_df = coffee_df.withColumn('volume_filter_100', when(coffee_df.Volume >= 100, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# Making sure column was properly created\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == True).show(5)\n",
    "coffee_df.filter(coffee_df.volume_filter_100 == False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Open_Close_Abs|\n",
      "+--------------+\n",
      "|          5.75|\n",
      "|           0.0|\n",
      "|           3.6|\n",
      "|          2.15|\n",
      "|           3.1|\n",
      "|          5.95|\n",
      "|           2.3|\n",
      "|          1.15|\n",
      "|           0.7|\n",
      "|           5.2|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Absolute Value column\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "coffee_df = coffee_df.withColumn('Open_Close_Abs', abs(coffee_df.Open_Close_Diff))\n",
    "\n",
    "# Show Absolute Value column\n",
    "coffee_df.select('Open_Close_Abs').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|net_values|\n",
      "+----------+\n",
      "|      1838|\n",
      "|      6182|\n",
      "|     11605|\n",
      "|      3811|\n",
      "|      1195|\n",
      "|      7243|\n",
      "|    241355|\n",
      "|      3007|\n",
      "|      2516|\n",
      "|    182294|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Create udf\n",
    "@pandas_udf('long')\n",
    "def compute_net(c1: pd.Series,\n",
    "                c2: pd.Series,\n",
    "                c3: pd.Series, \n",
    "                c4: pd.Series, \n",
    "                c5: pd.Series) -> float:\n",
    "  return ((c1 + c2 + c3 + c4) / 4) * c5\n",
    "\n",
    "# Create Winow\n",
    "w = Window \\\n",
    "    .partitionBy('Open', 'High', 'Close', 'Low') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "# Create net_values column\n",
    "coffee_df = coffee_df.withColumn('net_values', compute_net(coffee_df.Open, coffee_df.High, coffee_df.Low, coffee_df.Close, coffee_df.Volume).over(w))\n",
    "\n",
    "# Make sure net_values column is there \n",
    "coffee_df.select('net_values').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "avg() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7055/3084688768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create net_sales column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcoffee_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net_sales'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoffee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_sales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: avg() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Create net_sales column\n",
    "coffee_df = coffee_df.withColumn('net_sales', avg(coffee_df.Open, coffee_df.High, coffee_df.Low, coffee_df.Close * coffee_df.Volume))\n",
    "\n",
    "coffee_df.select(coffee_df.net_sales).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b317e639739deb127efa3a357daeb3681244e8477c1f3da0e4c6730da0730415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
